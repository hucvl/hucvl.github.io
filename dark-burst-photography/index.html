<!doctype html>

<head>
  <title>Burst Photography for Learning to Enhance Extremely Dark Images</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <script src="./dist/template.v2.js"></script>


  <style>.subgrid {
    grid-column: screen;
    display: grid;
    grid-template-columns: inherit;
    grid-template-rows: inherit;
    grid-column-gap: inherit;
    grid-row-gap: inherit;
  }

  d-figure.base-grid {
    grid-column: screen;
    background: hsl(0, 0%, 97%);
    padding: 20px 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  d-figure {
    margin-bottom: 1em;
    position: relative;
  }

  d-figure > figure {
    margin-top: 0;
    margin-bottom: 0;
  }
  d-title h1{
    font-size: 40px;
  }

  content.l-body-outset{

    width: 300px;
  }

  .shaded-figure {
    background-color: hsl(0, 0%, 97%);
    border-top: 1px solid hsla(0, 0%, 0%, 0.1);
    border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
    padding: 30px 0;
  }

  .pointer {
    position: absolute;
    width: 26px;
    height: 26px;
    top: 26px;
    left: -48px;
  }

  .figure-element, .figure-line, .figure-path {
    stroke: #666;
    stroke-miterlimit: 10px;
    stroke-width: 1.5px;
  }

  .figure-element {
    fill: #fff;
    fill-opacity: 0.8;
  }

  .figure-line {
    fill: none;
  }

  .figure-path {
    fill: #666;
    stroke-width: 1px;
  }

  .figure-group {
    fill: #f9f9f9;
    stroke: #666;
    stroke-width: 1.5px;
    stroke-opacity: 0.6;
    stroke-miterlimit: 10px;
  }

  .figure-faded {
    opacity: 0.35;
  }

  .figure-box {
    rx: 6px;
    ry: 6px;
  }

  .figure-dashed {
    stroke: #666;
    stroke-width: 1.5px;
    stroke-miterlimit: 10px;
    stroke-dasharray: 5, 5;
  }

  .figure-text {
    fill: #000;
    opacity: 0.6;
    font-size: 13px;
  }

  .figure-text-faded {
    opacity: 0.35;
  }

  .figure-large-text {
    font-size: 18px;
  }

  .subscript {
    font-size: 8px;
  }

  .figure-film-generator {
    /* stroke: #006064; */
    /* fill: #80DEEA; */

    stroke: hsl(203, 65%, 70%);
    fill: hsl(203, 65%, 85%);
  }

  .figure-film-generator-shaded {
    /* stroke: #006064; */
    /* fill: #00838F; */

    stroke: hsl(203, 15%, 85%);
    fill: hsl(203, 15%, 95%);
  }

  .figure-filmed-network {
    /* stroke: #BF360C; */
    /* fill: #FFAB91; */

    stroke: hsl(11, 65%, 70%);
    fill: rgb(242, 203, 194);
  }

  .todo {
    color: red;
  }


  .tooltip {
    position: absolute;
    max-width: 300px;
    max-height: 300px;
    pointer-events: none;
    transition: opacity;
  }

  .collapsible {
    cursor: pointer;
    padding-top: 12px;
    padding-bottom: 12px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
    font-weight: 700;
    background-color: white;
    color: rgba(0, 0, 0, 0.8);
    padding: 0.5em;
    margin: 0.2em;
    margin-left: 0;
    padding-left: 0;
    transform: translateX(0px);
    transition:
            color 0.1s ease-out,
            transform 0.25s ease;
  }

  .collapsible:hover {
    border-bottom: 1px solid inset;
    color: rgba(0, 0, 0, 0.4);
    transform: translateX(10px);
    transition:
            transform 0.25s ease;
  }

  d-article .content {
    display: none;
    overflow: hidden;
    background-color: none;
  }

  .expand-collapse-button {
    cursor: pointer;
    border: none;
    outline: none;
    font-size: 18px;
    font-weight: 700;
    float: right;
  }


  #clevr-plot-svg {
    width:440px;
    height:400px
  }

  #style-transfer-plot-svg {
    width:440px;
    height:400px
  }



iframe, object, embed {
  width: 100%;
  height:400px
  display: block !important;
}
  </style>
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>
  <script src="https://ariutta.github.io/svg-pan-zoom/dist/svg-pan-zoom.js"></script>

</head>

<body>

<d-front-matter>
  <script type="text/json">{
    "title": "Burst Photography for Learning to Enhance Extremely Dark Images",
    "description": "A new burst-based image enhancement method for extremely low-light images.",
    "authors": [
      {
        "author": "Ahmet Serdar Karadeniz",
        "authorURL": "https://askaradeniz.github.io/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      },
      {
        "author": "Erkut Erdem",
        "authorURL": "https://web.cs.hacettepe.edu.tr/~erkut/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      },
      {
        "author": "Aykut Erdem",
        "authorURL": "https://aykuterdem.github.io",
        "affiliations": [{"name": "Koç University", "url": "https://www.ku.edu.tr"}]
      }
    ],
    "katex": {
      "delimiters": [
        {
          "left": "$",
          "right": "$",
          "display": false
        },
        {
          "left": "$$",
          "right": "$$",
          "display": true
        }
      ]
    }
  }</script>
</d-front-matter>

<d-title>
  <h1>Burst Photography for Learning to Enhance Extremely Dark Images</h1>
  <p>A new image enhancement method for extremely low-light images.</p>
  <div class="l-page" id="vtoc"></div>
</d-title>

<d-article>
  <figure >
    <img src="./images/teaser.png">
    <figcaption>A sample result obtained with our proposed burst-based extremely low-light image enhancement method. The standard camera output and its scaled version are shown at the top left corner. For comparison, the zoomed-in details from the outputs produced by the existing approaches are given in the subfigures. The results of the single image enhancement models, denoted with (S), are shown on the right. The results of the multiple image enhancement methods are presented at the bottom, with (B) denoting the burst and (E) indicating the ensemble models. Our single image model recovers finer-scale details much better than its state-of-the-art counterparts. Moreover, our burst model gives perceptually the most satisfactory result, compared to all the other methods.</figcaption>
  </figure>

  <div>
  <a href="#.pdf"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper.png" width=190></a>
  <br>
  
  <b>Paper</b>   
  <p>Ahmet Serdar Karadeniz, Erkut Erdem, and Aykut Erdem. "Burst Photography for Learning to Enhance Extremely Dark Images", IEEE Transactions on Image Processing, in press.
    <br>
    <a href="https://web.cs.hacettepe.edu.tr/~erkut/publications/dark-burst-photography.pdf">Paper (high-res)</a> | <a href="https://web.cs.hacettepe.edu.tr/~erkut/publications/dark-burst-photography-lowres.pdf">Paper (low-res)</a> | <a href="bibtex.txt">Bibtex</a>
  </p>


  <b>Code:</b> <a href='http://github.com/hucvl/dark-burst-photography'>Tensorflow implementation</a></b>
  <br>
  </div>
  <hr/>

  <h2 id="abstract">Abstract</h2>
  <p>
    Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. 
    Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. 
    Recently, learning-based approaches have shown very promising results for this task since they have substantially 
    more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to 
    leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from 
    extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture 
    that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, 
    which is then fed to the fine network to recover fine- scale details and realistic textures. To further reduce 
    the noise level and improve the color accuracy, we extend this network to a permutation invariant structure 
    so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. 
    Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art 
    methods by producing more detailed and considerably higher quality images.

  </p>

  <h2 id="introduction">Introduction</h2>
  
  <p>
    Capturing images in low-light conditions is a challenging task -- the main difficulty being that the level of the signal
    measured by the camera sensors is generally much lower than the noise in the measurements <d-cite key="Liba2019"></d-cite>. The fundamental factors
    causing the noise are the variations in the number of photons entering the camera lens and the sensor-based measurement
    errors occurred when reading the signal <d-cite key="Hasinoff2014,Brooks2019"></d-cite>. In addition, noise present in a low-light image also affects various
    image characteristics such as fine-scale structures and color balance, further degrading the image quality.
  </p>

  <p>
    While the previous methods <d-cite key="Chen2018,Zamir2019a,Maharjan2019a"></d-cite> obtain an RGB image from a single dark raw image, we further explore whether the results
    can be improved by integrating multiple observations regarding the scene. Despite the remarkable progress of previous studies, 
    there is still large room for improvement, regarding various issues such as unwanted blur, noise and color inaccuracies in the end 
    results – especially for the input images which are extremely dark. In a nutshell, to alleviate these shortcomings, in this study,
    we propose a learning-based framework that takes a burst of extremely low-light raw images of a scene as input and generates an enhanced
    RGB image. The use of burst images has been previously investigated before <d-cite key="Ma2020"></d-cite>. That said, in our work, we develop a coarse-to-fine network architecture which allows for simultaneous processing of a burst of dark
    raw images as input to obtain a high quality RGB image.
  </p>

  <h2>System Overview</h2>
  <figure >
    <img src="./images/proposed_method.png">
    <figcaption>Network architectures of the proposed single-frame coarse-to-fine model (left), and set-based burst model (right).</figcaption>
  </figure>

  <p>
    To recover fine-grained details from dark images, we propose to employ a two-step coarse-to-fine training procedure.
    Our coarse network outputs a denoised image in rawRGB space. We utilize the output of the coarse network not just for
    guidance in assisting the fine network but also in approximating the noise by computing the difference between the upsampled
    coarse prediction and the raw low-light input. The fine network takes the concatenation of the low-light raw input image, the
    output from the coarse network and the noise approximation as inputs and processes them to generate the final RGB output.
  </p>

  <p>
    We extend our coarse-to-fine model to a novel permutation invariant CNN architecture which takes multiple images of the scene as
    input and predicts an enhanced image. In particular, first, low-resolution coarse outputs are obtained for each frame in the burst
    sequence, using our coarse network. Then, our set-based network accepts a set of tensors as input, each instance corresponding to the
    concatenation of one of raw burst images, its noise approximation and the upsampled version of the coarse prediction and produces final output.
  </p>

  <figure >
    <img src="./images/motivation.png">
    <figcaption>An example night photo captured with 0.1 sec exposure and its enhanced versions by the proposed coarse, fine and burst networks. As the cropped images demonstrate, the fine network enhances both the color and the details of the coarse result. The burst network produces even much sharper and perceptually more pleasing output.</figcaption>
  </figure>

  <p>
    To obtain robustness to small motions, we apply max fusion between the features of burst frames after the second convolution block.
    As the features are downsampled, their alignment becomes much easier and the network benefits from the fusion of the higher-level features.
    To deal with large motions in the scene, however, we can utilize the outputs of our coarse network to estimate optical flows between consecutive frames.
    In our experiments, we employ the method in <d-cite key="teed2020raft"></d-cite> to obtain the optical flows, which are then used to compensate motion by selectively performing fusion 
    at the input level only over the regions with little or no motion. We also compare our model with the Seeing Motion in the Dark (SMID) method of Chen et al. on the DRV dataset <d-cite key="chen2019seeing"></d-cite>.
  </p>

  <h2>Results</h2>
  <h3>Single Image Results</h3>

  <div style="float: right;">
      <select class="cmp_option_single">
        <option>Traditional + Scaling vs. Ours (single)</option>
        <option>SID (2018) vs. Ours (single)</option>
        <option>Zamir et al. (2019) vs. Ours (single)</option>
        <option>Maharjan et al. (2019) vs. Ours (single)</option>
      </select>
  </div>
  <br>
  <br>
  <div class="compare_single">
          
          <div>
                <p><small>Sony a7s II, ISO 12800 1/10s</small></p>
                <div class="cocoen scale single" style="width: 100%;">
                  <img src="./images/cmp_single/10213_00_100_scale.jpg" />
                  <img src="./images/cmp_single/10213_00_100_ours.jpg" />
                </div>

                <div class="cocoen sid single" style="width: 100%;">
                  <img src="./images/cmp_single/10213_00_100_sid.jpg" />
                  <img src="./images/cmp_single/10213_00_100_ours.jpg" />
                </div>

                <div class="cocoen maharjan single" style="width: 100%;">
                  <img src="./images/cmp_single/10213_00_100_maharjan.jpg" />
                  <img src="./images/cmp_single/10213_00_100_ours.jpg" />
                </div>

                <div class="cocoen zamir single" style="width: 100%;">
                  <img src="./images/cmp_single/10213_00_100_zamir.jpg" />
                  <img src="./images/cmp_single/10213_00_100_ours.jpg" />
                </div>
        </div>
        <br><br>
        <div>
              <p><small>iPhone 6s, ISO 400 1/20s</small></p>
              <div class="cocoen scale single" style="width: 100%;">
                <img src="./images/cmp_single/iphone6s_scale.jpg" />
                <img src="./images/cmp_single/iphone6s_ours.jpg" />
              </div>

              <div class="cocoen sid single" style="width: 100%;">
                <img src="./images/cmp_single/iphone6s_sid.jpg" />
                <img src="./images/cmp_single/iphone6s_ours.jpg" />
              </div>

              <div class="cocoen maharjan single" style="width: 100%;">
                <img src="./images/cmp_single/iphone6s_maharjan.jpg" />
                <img src="./images/cmp_single/iphone6s_ours.jpg" />
              </div>

              <div class="cocoen zamir single" style="width: 100%;">
                <img src="./images/cmp_single/iphone6s_zamir.jpg" />
                <img src="./images/cmp_single/iphone6s_ours.jpg" />
              </div>
        </div>
         
        
          

        
          


  </div>

  <h3>Burst Results</h3>

  <div style="float: right;">
      <select class="cmp_option_burst">
        <option>Traditional + Scaling vs. Ours (burst)</option>
        <option>SID (2018) vs. Ours (burst)</option>
        <option>Zamir et al. (2019) vs. Ours (burst)</option>
        <option>Maharjan et al. (2019) vs. Ours (burst)</option>
        <option>Ma et al. (2020) vs. Ours (burst)</option>
      </select>
  </div>
  <br>
  <br>
  <div class="compare_burst">
          
          <div>
                <p><small>Sony a7s II, ISO 640 1/10s, 8 Frames</small></p>
                <div class="cocoen scale multi" style="width: 100%;">
                  <img src="./images/cmp_burst/10082_00_300_scale.jpg" />
                  <img src="./images/cmp_burst/10082_00_300_ours.jpg" />
                </div>

                <div class="cocoen sid multi" style="width: 100%;">
                  <img src="./images/cmp_burst/10082_00_300_sid.jpg" />
                  <img src="./images/cmp_burst/10082_00_300_ours.jpg" />
                </div>

                <div class="cocoen maharjan multi" style="width: 100%;">
                  <img src="./images/cmp_burst/10082_00_300_maharjan.jpg" />
                  <img src="./images/cmp_burst/10082_00_300_ours.jpg" />
                </div>

                <div class="cocoen zamir multi" style="width: 100%;">
                  <img src="./images/cmp_burst/10082_00_300_zamir.jpg" />
                  <img src="./images/cmp_burst/10082_00_300_ours.jpg" />
                </div>

                <div class="cocoen ma multi" style="width: 100%;">
                  <img src="./images/cmp_burst/10082_00_300_ma.jpg" />
                  <img src="./images/cmp_burst/10082_00_300_ours.jpg" />
                </div>
        </div>
        <br><br>
        <div>
              <p><small>Sony a7s II, ISO 1600 1/10s, 8 Frames</small></p>
              <div class="cocoen scale multi" style="width: 100%;">
                <img src="./images/cmp_burst/10074_00_300_scale.jpg" />
                <img src="./images/cmp_burst/10074_00_300_ours.jpg" />
              </div>

              <div class="cocoen sid multi" style="width: 100%;">
                <img src="./images/cmp_burst/10074_00_300_sid.jpg" />
                <img src="./images/cmp_burst/10074_00_300_ours.jpg" />
              </div>

              <div class="cocoen maharjan multi" style="width: 100%;">
                <img src="./images/cmp_burst/10074_00_300_maharjan.jpg" />
                <img src="./images/cmp_burst/10074_00_300_ours.jpg" />
              </div>

              <div class="cocoen zamir multi" style="width: 100%;">
                <img src="./images/cmp_burst/10074_00_300_zamir.jpg" />
                <img src="./images/cmp_burst/10074_00_300_ours.jpg" />
              </div>

              <div class="cocoen ma multi" style="width: 100%;">
                <img src="./images/cmp_burst/10074_00_300_ma.jpg" />
                <img src="./images/cmp_burst/10074_00_300_ours.jpg" />
              </div>
        </div>
         



  </div>

  <h3>Video Results</h3>

  <div style="float: right;">
      <select class="cmp_option_video">
        <option>Ours</option>
        <option>Chen et al. (2019)</option>
      </select>
  </div>
  <br>
  <br>
  
  <div class="compare_video">
    <div class="video ours" style="width: 100%;">
      <video width="100%" controls>
        <source src="./video/ours/M0001.avi" type="video/avi">
      </video>
    </div>

    <div class="video chen" style="width: 100%;">
      <video width="100%" controls>
        <source src="./video/chen/M0001.avi" type="video/avi">

      </video>
    </div>
  </div>

  <div class="compare_video">
    <div class="video ours" style="width: 100%;">
      <video width="100%" controls>
        <source src="./video/ours/M0019.avi" type="video/avi">
      </video>
    </div>

    <div class="video chen" style="width: 100%;">
      <video width="100%" controls>
        <source src="./video/chen/M0019.avi" type="video/avi">

      </video>
    </div>
  </div>
    


  <hr/>

</d-article>

<d-appendix>

  <h3 id="acknowledgements">Acknowledgements</h3>
  <p>
    This work was supported in part by GEBIP 2018 Award of the Turkish Academy of Sciences to E. Erdem, BAGEP 2021 Award of the Science Academy to A. Erdem. We would like to thank KUIS AI Center for letting us use their High Performance Computing Cluster and  NVIDIA Corporation for the donation of GPUs used in this research. 
  </p>

 <!-- <h3 id="peer-reviews">Discussion and Review</h3>
  <p>
    <a href="#">Review 1 - Anonymous </a><br>
   <a href="#">Review 2 - Anonymous </a><br>
    <a href="#">Review 3 - Anonymous</a><br>
  </p>-->

  <!--<d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>-->
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering-->
<d-bibliography src="bibliography.bib"></d-bibliography>



<script type="text/javascript" src="index.bundle.js"></script></body>

<link rel="stylesheet" href="css/cocoen.min.css" />
<script type="text/javascript" src="./js/cocoen-jquery.js"></script>
<script type="text/javascript" src="./js/cocoen.js"></script>


<script   src="https://code.jquery.com/jquery-3.5.1.slim.js"   integrity="sha256-DrT5NfxfbHvMHux31Lkhxg42LY6of8TaYyK50jnxRnM="   crossorigin="anonymous"></script>


<script>

  $(document).ready(function(){
    // Initialize cocoen.
    $('.cocoen').cocoen();
  });

  document.addEventListener('DOMContentLoaded', function(){

      // Add slider elements.
      document.querySelectorAll('.cocoen').forEach(function(element){
          new Cocoen(element);
      });

      $("div.cocoen.sid").hide()
      $("div.cocoen.maharjan").hide()
      $("div.cocoen.zamir").hide()
      $("div.cocoen.ma").hide()
      $("div.video.chen").hide()

  });
  
  // Single image option changed.
  $(document).ready(function(){
    $(".cmp_option_single").change(function(){
      var prev = $(this).data('val');
      var selectedStr = $(this).val()
      updateSliderLeftImage(prev, selectedStr, true)

    });

    // Burst image option changed.
    $(".cmp_option_burst").change(function(){
      var prev = $(this).data('val');
      var selectedStr = $(this).val()
      updateSliderLeftImage(prev, selectedStr, false)

    });

    // Video option changed.
    $(document).ready(function(){
    $(".cmp_option_video").change(function(){
      var prev = $(this).data('val');
      var selectedStr = $(this).val()
      updateVideo(prev, selectedStr)
    });


  });



  });

</script>

<script>
  // Update left part of the slider.
  // prev: previous method name, selectedStr: new method name, single: single or burst
  function updateSliderLeftImage(prev, selectedStr, single){
    
    var methodName = "scale"
    if(selectedStr.includes("Traditional + Scale")){
        methodName = "scale"
    }
    else if(selectedStr.includes("SID")){
        methodName = "sid"
    }
    else if(selectedStr.includes("Maharjan et al.")){
        methodName = "maharjan"
    }
    else if(selectedStr.includes("Zamir et al.")){
        methodName = "zamir"
    }
    else if(selectedStr.includes("Ma et al.")){
        methodName = "ma"
    }


    if(single == false){
      $('div.cocoen.multi').hide()
      var slider = $('div.cocoen.'+methodName+".multi")
      slider.show()
    }
    else{
      $('div.cocoen.single').hide()
      var slider = $('div.cocoen.'+methodName+".single")
      slider.show()
    }
    
}

  function updateVideo(prev, selectedStr){
    
      var methodName = "ours"
      if(selectedStr.includes("Ours")){
          methodName = "ours"
      }
      else if(selectedStr.includes("Chen et al. (2019)")){
          methodName = "chen"
      }

      $('div.video').hide()
      var vid = $('div.video.'+methodName)
      vid.show()
    
  }
</script>
