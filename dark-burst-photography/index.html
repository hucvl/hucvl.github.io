<!doctype html>

<head>
  <title>Manipulating Attributes of Natural Scenes via Hallucination</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <script src="deneme.js"></script>
  <script src="./dist/template.v2.js"></script>


  <style>.subgrid {
    grid-column: screen;
    display: grid;
    grid-template-columns: inherit;
    grid-template-rows: inherit;
    grid-column-gap: inherit;
    grid-row-gap: inherit;
  }

  d-figure.base-grid {
    grid-column: screen;
    background: hsl(0, 0%, 97%);
    padding: 20px 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  d-figure {
    margin-bottom: 1em;
    position: relative;
  }

  d-figure > figure {
    margin-top: 0;
    margin-bottom: 0;
  }
  d-title h1{
    font-size: 40px;
  }

  content.l-body-outset{

    width: 300px;
  }

  .shaded-figure {
    background-color: hsl(0, 0%, 97%);
    border-top: 1px solid hsla(0, 0%, 0%, 0.1);
    border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
    padding: 30px 0;
  }

  .pointer {
    position: absolute;
    width: 26px;
    height: 26px;
    top: 26px;
    left: -48px;
  }

  .figure-element, .figure-line, .figure-path {
    stroke: #666;
    stroke-miterlimit: 10px;
    stroke-width: 1.5px;
  }

  .figure-element {
    fill: #fff;
    fill-opacity: 0.8;
  }

  .figure-line {
    fill: none;
  }

  .figure-path {
    fill: #666;
    stroke-width: 1px;
  }

  .figure-group {
    fill: #f9f9f9;
    stroke: #666;
    stroke-width: 1.5px;
    stroke-opacity: 0.6;
    stroke-miterlimit: 10px;
  }

  .figure-faded {
    opacity: 0.35;
  }

  .figure-box {
    rx: 6px;
    ry: 6px;
  }

  .figure-dashed {
    stroke: #666;
    stroke-width: 1.5px;
    stroke-miterlimit: 10px;
    stroke-dasharray: 5, 5;
  }

  .figure-text {
    fill: #000;
    opacity: 0.6;
    font-size: 13px;
  }

  .figure-text-faded {
    opacity: 0.35;
  }

  .figure-large-text {
    font-size: 18px;
  }

  .subscript {
    font-size: 8px;
  }

  .figure-film-generator {
    /* stroke: #006064; */
    /* fill: #80DEEA; */

    stroke: hsl(203, 65%, 70%);
    fill: hsl(203, 65%, 85%);
  }

  .figure-film-generator-shaded {
    /* stroke: #006064; */
    /* fill: #00838F; */

    stroke: hsl(203, 15%, 85%);
    fill: hsl(203, 15%, 95%);
  }

  .figure-filmed-network {
    /* stroke: #BF360C; */
    /* fill: #FFAB91; */

    stroke: hsl(11, 65%, 70%);
    fill: rgb(242, 203, 194);
  }

  .todo {
    color: red;
  }


  .tooltip {
    position: absolute;
    max-width: 300px;
    max-height: 300px;
    pointer-events: none;
    transition: opacity;
  }

  .collapsible {
    cursor: pointer;
    padding-top: 12px;
    padding-bottom: 12px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
    font-weight: 700;
    background-color: white;
    color: rgba(0, 0, 0, 0.8);
    padding: 0.5em;
    margin: 0.2em;
    margin-left: 0;
    padding-left: 0;
    transform: translateX(0px);
    transition:
            color 0.1s ease-out,
            transform 0.25s ease;
  }

  .collapsible:hover {
    border-bottom: 1px solid inset;
    color: rgba(0, 0, 0, 0.4);
    transform: translateX(10px);
    transition:
            transform 0.25s ease;
  }

  d-article .content {
    display: none;
    overflow: hidden;
    background-color: none;
  }

  .expand-collapse-button {
    cursor: pointer;
    border: none;
    outline: none;
    font-size: 18px;
    font-weight: 700;
    float: right;
  }


  #clevr-plot-svg {
    width:440px;
    height:400px
  }

  #style-transfer-plot-svg {
    width:440px;
    height:400px
  }


iframe, object, embed {
  width: 100%;
  height:400px
  display: block !important;
}
  </style>
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>
  <script src="https://ariutta.github.io/svg-pan-zoom/dist/svg-pan-zoom.js"></script>

</head>

<body onload="initDemo()">

<d-front-matter>
  <script type="text/json">{
    "title": "Burst Photography for Learning to Enhance Extremely Dark Images",
    "description": "A new image enhancement method for extremely low-light images.",
    "authors": [
      {
        "author": "Ahmet Serdar Karadeniz",
        "authorURL": "https://askaradeniz.github.io/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      },
      {
        "author": "Erkut Erdem",
        "authorURL": "https://web.cs.hacettepe.edu.tr/~erkut/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      },
      {
        "author": "Aykut Erdem",
        "authorURL": "https://web.cs.hacettepe.edu.tr/~aykut/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      }
    ],
    "katex": {
      "delimiters": [
        {
          "left": "$",
          "right": "$",
          "display": false
        },
        {
          "left": "$$",
          "right": "$$",
          "display": true
        }
      ]
    }
  }</script>
</d-front-matter>

<d-title>
  <h1>Burst Photography for Learning to Enhance Extremely Dark Images</h1>
  <p>A new image enhancement method for extremely low-light images.</p>
  <div class="l-page" id="vtoc"></div>
</d-title>

<d-article>
  <figure >
    <img src="./images/teaser.png">
    <figcaption>A sample result obtained with our proposed burst- based extremely low-light image enhancement method. The standard camera output and its scaled version are shown at the top left corner. For comparison, the zoomed-in details from the outputs produced by the existing approaches are given in the subfigures. The results of the single image enhancement models, denoted with (S), are shown on the right. The results of the multiple image enhancement methods are presented at the bottom, with (B) denoting the burst and (E) indicating the ensemble models. Our single image model recovers finer-scale details much better than its state-of-the-art counterparts. Moreover, our burst model gives perceptually the most satisfactory result, compared to all the other methods.</figcaption>
  </figure>

  <div>
  <a href="#.pdf"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper.png" width=190></a>
  <br>
  
  <b>Paper</b>   
  <p>Ahmet Serdar Karadeniz, Erkut Erdem, and Aykut Erdem. "Burst Photography for Learning to Enhance Extremely Dark Images", submitted.
    <br>
    <a href="#.pdf">Paper</a> | <a href="bibtex.txt">Bibtex</a>
  </p>


  <b>Code and Dataset:</b> <a href='http://github.com/hucvl/dark-burst-photography'>PyTorch implementation</a></b>
  <br>
  </div>
  <hr/>

  <h2 id="abstract">Abstract</h2>
  <p>
    Capturing images under extremely low-light con- ditions poses significant challenges for the standard camera pipeline. 
    Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. 
    Recently, learning-based approaches have shown very promising results for this task since they have substantially 
    more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to 
    leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from 
    extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture 
    that generates high-quality outputs progres- sively. The coarse network predicts a low-resolution, denoised raw image, 
    which is then fed to the fine network to recover fine- scale details and realistic textures. To further reduce 
    the noise level and improve the color accuracy, we extend this network to a permutation invariant structure 
    so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. 
    Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art 
    methods by producing more detailed and considerably higher quality images.

  </p>

  <h2 id="introduction">Introduction</h2>
  
   <p>
    ddd
  </p>

  <p>
    ddd
  </p>

  <h2>System Overview</h2>

  <figure>
    <img src="./images/system_overview.jpg">
    <figcaption>ddd</figcaption>
  </figure>
  <p>
    ddd
  </p>

  


  



   <!-- <d-cite key="perez2018film"></d-cite>-->

  <hr/>

</d-article>

<d-appendix>

  <h3 id="acknowledgements">Acknowledgements</h3>
  <p>
    This work was supported in part by TUBA GEBIP fellowship awarded to E. Erdem. We would like to thank NVIDIA Corporation for the donation of GPUs used in this research. 
  </p>

 <!-- <h3 id="peer-reviews">Discussion and Review</h3>
  <p>
    <a href="#">Review 1 - Anonymous </a><br>
   <a href="#">Review 2 - Anonymous </a><br>
    <a href="#">Review 3 - Anonymous</a><br>
  </p>-->

  <!--<d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>-->
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering-->
<d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>
