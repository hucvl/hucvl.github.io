<!doctype html>

<head>
  <title>Burst Photography for Learning to Enhance Extremely Dark Images</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <script src="./dist/template.v2.js"></script>


  <style>.subgrid {
    grid-column: screen;
    display: grid;
    grid-template-columns: inherit;
    grid-template-rows: inherit;
    grid-column-gap: inherit;
    grid-row-gap: inherit;
  }

  d-figure.base-grid {
    grid-column: screen;
    background: hsl(0, 0%, 97%);
    padding: 20px 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  d-figure {
    margin-bottom: 1em;
    position: relative;
  }

  d-figure > figure {
    margin-top: 0;
    margin-bottom: 0;
  }
  d-title h1{
    font-size: 40px;
  }

  content.l-body-outset{

    width: 300px;
  }

  .shaded-figure {
    background-color: hsl(0, 0%, 97%);
    border-top: 1px solid hsla(0, 0%, 0%, 0.1);
    border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
    padding: 30px 0;
  }

  .pointer {
    position: absolute;
    width: 26px;
    height: 26px;
    top: 26px;
    left: -48px;
  }

  .figure-element, .figure-line, .figure-path {
    stroke: #666;
    stroke-miterlimit: 10px;
    stroke-width: 1.5px;
  }

  .figure-element {
    fill: #fff;
    fill-opacity: 0.8;
  }

  .figure-line {
    fill: none;
  }

  .figure-path {
    fill: #666;
    stroke-width: 1px;
  }

  .figure-group {
    fill: #f9f9f9;
    stroke: #666;
    stroke-width: 1.5px;
    stroke-opacity: 0.6;
    stroke-miterlimit: 10px;
  }

  .figure-faded {
    opacity: 0.35;
  }

  .figure-box {
    rx: 6px;
    ry: 6px;
  }

  .figure-dashed {
    stroke: #666;
    stroke-width: 1.5px;
    stroke-miterlimit: 10px;
    stroke-dasharray: 5, 5;
  }

  .figure-text {
    fill: #000;
    opacity: 0.6;
    font-size: 13px;
  }

  .figure-text-faded {
    opacity: 0.35;
  }

  .figure-large-text {
    font-size: 18px;
  }

  .subscript {
    font-size: 8px;
  }

  .figure-film-generator {
    /* stroke: #006064; */
    /* fill: #80DEEA; */

    stroke: hsl(203, 65%, 70%);
    fill: hsl(203, 65%, 85%);
  }

  .figure-film-generator-shaded {
    /* stroke: #006064; */
    /* fill: #00838F; */

    stroke: hsl(203, 15%, 85%);
    fill: hsl(203, 15%, 95%);
  }

  .figure-filmed-network {
    /* stroke: #BF360C; */
    /* fill: #FFAB91; */

    stroke: hsl(11, 65%, 70%);
    fill: rgb(242, 203, 194);
  }

  .todo {
    color: red;
  }


  .tooltip {
    position: absolute;
    max-width: 300px;
    max-height: 300px;
    pointer-events: none;
    transition: opacity;
  }

  .collapsible {
    cursor: pointer;
    padding-top: 12px;
    padding-bottom: 12px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
    font-weight: 700;
    background-color: white;
    color: rgba(0, 0, 0, 0.8);
    padding: 0.5em;
    margin: 0.2em;
    margin-left: 0;
    padding-left: 0;
    transform: translateX(0px);
    transition:
            color 0.1s ease-out,
            transform 0.25s ease;
  }

  .collapsible:hover {
    border-bottom: 1px solid inset;
    color: rgba(0, 0, 0, 0.4);
    transform: translateX(10px);
    transition:
            transform 0.25s ease;
  }

  d-article .content {
    display: none;
    overflow: hidden;
    background-color: none;
  }

  .expand-collapse-button {
    cursor: pointer;
    border: none;
    outline: none;
    font-size: 18px;
    font-weight: 700;
    float: right;
  }


  #clevr-plot-svg {
    width:440px;
    height:400px
  }

  #style-transfer-plot-svg {
    width:440px;
    height:400px
  }


iframe, object, embed {
  width: 100%;
  height:400px
  display: block !important;
}
  </style>
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>
  <script src="https://ariutta.github.io/svg-pan-zoom/dist/svg-pan-zoom.js"></script>

</head>

<body>

<d-front-matter>
  <script type="text/json">{
    "title": "Burst Photography for Learning to Enhance Extremely Dark Images",
    "description": "A new image enhancement method for extremely low-light images.",
    "authors": [
      {
        "author": "Ahmet Serdar Karadeniz",
        "authorURL": "https://askaradeniz.github.io/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      },
      {
        "author": "Erkut Erdem",
        "authorURL": "https://web.cs.hacettepe.edu.tr/~erkut/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      },
      {
        "author": "Aykut Erdem",
        "authorURL": "https://web.cs.hacettepe.edu.tr/~aykut/",
        "affiliations": [{"name": "Hacettepe University", "url": "https://hacettepe.edu.tr"}]
      }
    ],
    "katex": {
      "delimiters": [
        {
          "left": "$",
          "right": "$",
          "display": false
        },
        {
          "left": "$$",
          "right": "$$",
          "display": true
        }
      ]
    }
  }</script>
</d-front-matter>

<d-title>
  <h1>Burst Photography for Learning to Enhance Extremely Dark Images</h1>
  <p>A new image enhancement method for extremely low-light images.</p>
  <div class="l-page" id="vtoc"></div>
</d-title>

<d-article>
  <figure >
    <img src="./images/teaser.png">
    <figcaption>A sample result obtained with our proposed burst- based extremely low-light image enhancement method. The standard camera output and its scaled version are shown at the top left corner. For comparison, the zoomed-in details from the outputs produced by the existing approaches are given in the subfigures. The results of the single image enhancement models, denoted with (S), are shown on the right. The results of the multiple image enhancement methods are presented at the bottom, with (B) denoting the burst and (E) indicating the ensemble models. Our single image model recovers finer-scale details much better than its state-of-the-art counterparts. Moreover, our burst model gives perceptually the most satisfactory result, compared to all the other methods.</figcaption>
  </figure>

  <div>
  <a href="#.pdf"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper.png" width=190></a>
  <br>
  
  <b>Paper</b>   
  <p>Ahmet Serdar Karadeniz, Erkut Erdem, and Aykut Erdem. "Burst Photography for Learning to Enhance Extremely Dark Images", submitted.
    <br>
    <a href="#.pdf">Paper</a> | <a href="bibtex.txt">Bibtex</a>
  </p>


  <b>Code:</b> <a href='http://github.com/hucvl/dark-burst-photography'>Tensorflow implementation</a></b>
  <br>
  </div>
  <hr/>

  <h2 id="abstract">Abstract</h2>
  <p>
    Capturing images under extremely low-light con- ditions poses significant challenges for the standard camera pipeline. 
    Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. 
    Recently, learning-based approaches have shown very promising results for this task since they have substantially 
    more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to 
    leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from 
    extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture 
    that generates high-quality outputs progres- sively. The coarse network predicts a low-resolution, denoised raw image, 
    which is then fed to the fine network to recover fine- scale details and realistic textures. To further reduce 
    the noise level and improve the color accuracy, we extend this network to a permutation invariant structure 
    so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. 
    Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art 
    methods by producing more detailed and considerably higher quality images.

  </p>

  <h2 id="introduction">Introduction</h2>
  
   <p>
    Capturing images in low-light conditions is a challenging task – the main difficulty being that the level of the signal measured by the camera sensors is generally much lower than the noise in the measurements [1]. The fundamental factors causing the noise are the variations in the number of photons entering the camera lens and the sensor-based measurement errors occurred when reading the signal [2], [3]. In addition, noise present in a low-light image also affects various image characteristics such as fine-scale structures and color balance, further degrading the image quality.
  </p>

  <p>
    In the context of enhancing extremely dark images, See-in-the-Dark (SID) [4] is the first learning-based attempt to replace the standard camera pipeline, training a convolutional neural network (CNN) model to produce an enhanced RGB image from a single raw low-light image. For this purpose, the authors collected a dataset of short-exposure, dark raw pho- tos and their corresponding long-exposure references. Their method is further improved by Maharjan et al. [5] and Zamir et al. [6] with some changes in the CNN architecture and the objective functions utilized in training. In a similar fashion, in our study, we develop a new multi-scale architecture for single image enhancement and use a different objective by combining contextual and pixel-wise losses. While the previous methods obtain an RGB image from a single dark raw image, we further explore whether the results can be improved by integrating multiple observations regarding the scene.
  </p>

  <p>
    Bracketing is a well-known technique in photography that relies on rapidly taking several shots of the same scene. These shots usually differ from each other in terms of some camera settings, e.g. exposure, which capture characteristics of the scene differently, and thus they can be used for applications like constructing a high dynamic range (HDR) image. A technique similar to exposure bracketing is shooting each frame in the burst sequence with a constant exposure [4]. To our interest, when shot with a constant short exposure under low-light, these images represent different dark, noisy realizations of the same scene. Naturally, they provide us multiple observations about the scene when compared to a single dark image. While simply averaging these images reduces noise, results are not always satisfactory. For this reason, different techniques are introduced to merge the temporal pixels in the burst sequence [1], [4], [7]-[9]. Among these approaches, [7]–[9] use learning-based methods to process burst images. In these studies, burst images are fed to a CNN either by concatenating through channels or in a recurrent fashion. In our case, we propose a radically different approach and show that processing these burst images in a permutation invariant manner is a simple yet more effective approach. The order of burst images does not affect the output, and accordingly a more accurate output can be obtained. The multiple image enhancement models, which either employ burst imagery or integrate ensemble of enhanced images, give superior results than their single image counterparts, yet they still suffer from artifacts such as over-smoothing, and fail to recover fine-scale details in the image. Despite the remarkable progress of previous studies [28]–[30], [36], there is still large room for improvement, regarding various issues such as unwanted blur, noise and color inaccuracies in the end results – especially for the input images which are extremely dark.
  </p>

  <p>
    In a nutshell, to alleviate these shortcomings, in this study, we propose a learning-based framework that takes a burst of extremely low-light raw images of a scene as input and generates an enhanced RGB image. In particular, we develop a coarse-to-fine network architecture which allows for simultaneous processing of a burst of dark raw images as input to obtain a high quality RGB image.
  </p>

  <h2>System Overview</h2>

  <figure>
    <img src="./images/system_overview.jpg">
    <figcaption>ddd</figcaption>
  </figure>
  <p>
    ddd
  </p>

  


  



   <!-- <d-cite key="perez2018film"></d-cite>-->

  <hr/>

</d-article>

<d-appendix>

  <h3 id="acknowledgements">Acknowledgements</h3>
  <p>
    This work was supported in part by TUBA GEBIP fellowship awarded to E. Erdem. We would like to thank NVIDIA Corporation for the donation of GPUs used in this research. 
  </p>

 <!-- <h3 id="peer-reviews">Discussion and Review</h3>
  <p>
    <a href="#">Review 1 - Anonymous </a><br>
   <a href="#">Review 2 - Anonymous </a><br>
    <a href="#">Review 3 - Anonymous</a><br>
  </p>-->

  <!--<d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>-->
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering-->
<d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>
